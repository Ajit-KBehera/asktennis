version: "3.9"

services:
  db:
    image: postgres:16-alpine
    container_name: asktennis_production_db
    restart: unless-stopped
    environment:
      POSTGRES_DB: asktennis_production
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      # Optimize for large datasets
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./data:/docker-entrypoint-initdb.d/data
    ports:
      - "5432:5432"
    # Memory and performance optimizations for 3M+ records
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    # PostgreSQL configuration for large datasets
    command: >
      postgres
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
      -c maintenance_work_mem=256MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=64MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_connections=200

  app:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: asktennis_production_app
    depends_on:
      - db
    restart: unless-stopped
    environment:
      NODE_ENV: production
      PORT: 3000
      DATABASE_URL: postgresql://postgres:postgres@db:5432/asktennis_production?sslmode=disable
      GROQ_API_KEY: ${GROQ_API_KEY}
      SPORTRADAR_API_KEY: ${SPORTRADAR_API_KEY}
      # Memory optimization for large data processing
      NODE_OPTIONS: "--max-old-space-size=4096"
    ports:
      - "3000:3000"
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    # Memory limits for large dataset processing
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 2G
    command: ["node", "server.js"]


volumes:
  postgres_data:
